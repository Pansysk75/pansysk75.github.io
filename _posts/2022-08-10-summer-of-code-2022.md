---
title: "Google Summer of Code 2022 - Stellar Group"
date: 2022-08-10
categories:
  - blog
tags:
  - Projects
  - HPX
toc: true
toc_label: "Table of Contents"  
toc_sticky: true
---

This post is about my contribution to the HPX Concurrency and Parallelism library, for GSoC 2022. 



## Overview

This summer, I joined Ste\|\|ar group as a contributor through [Google Summer of Code (GSoC)][GSOC]. I contributed to the [HPX project][HPX], which is a C++ Standard Library for Concurrency and Parallelism.  
My project? ***HPX Algorithm Performance Analysis & Optimization***

>To explain further:  
>The C++ standard library defines functions for commonly used algorithms (eg. sorting, searching).
>HPX provides sequential and parallel implementations for all these algorithms.  
>My GSoC project involved performance analysis and optimization for these implementations.



## Completed Work

**Performance analysis with custom benchmarks**  
+ Writing small C++ benchmark programs for different algorithms / implementations.
+ Building & running them while changing various different parameters (workload, CPU cores, grain size).
+ Performing runs on different machines & configurations
+ Collecting timing data and creating useful visulizations using various metrics (i.e. strong/weak scaling, thread utilization charts)
+ Automating all above proccess for quick turnaround time

**Use of profiling tools**  
+ Instrumenting existing code for profiling with Intel VTune
+ Using existing HPX utilities or adding new as needed 

**Observation & new implementations**  
+ Using collected data to make observations, find bottlenecks and modify existing implementations.
+ No PRs yet, but made many interesting (expected and unexpected) observations. Laid the foundation for work to follow.



## Examples

Here are a few examples of the current state of my work:


### Basic Visualization

![showcase1](/assets/images/2022-08-10-summer-of-code-2022/showcase1.jpg)

Above are two fundamental visualizations: **Speedup Graph** and **Strong Scaling**.

> For all performance metrics, it's **crucial** to take a lot of samples, then take the average (see next section). A single measurement will not be accurate!

***Speedup Graph***  
The Speedup Graph shows the speedup (duh!) of our parallel ("par") implementations compared to the sequential one ("seq").
For a constant workload, speedup is defined as follows:

$$ speedup = \frac{\text{seq algorithm time}}{\text{par algorithm time}} $$

We can see that for small workloads ( < 10^5 ) , the speedup is < 1 .
That means that, for small workload, **parallel implementations are often slower that sequential ones**!

***Strong Scaling***  
The Strong Scaling graph shows us how speedup scales when we throw in more cores, for a constant workload.  
Ideally, speedup would increase proportionally to the number of cores (for example, 20 cores -> speedup of 20). This is however never the case, due to parallelization overhead, inefficiencies, and additional work that a parallel implementation may require. 


<!-- ### idk what this is

![showcase2](/assets/images/2022-08-10-summer-of-code-2022/showcase2.jpg) -->


### Individual data points

![scatter](/assets/images/2022-08-10-summer-of-code-2022/speedup_with_scatter.png)

Later on, we realized that being seeing the individual datapoints (instead of just the average-line) can be very valuable.  
In the above example, notice the bi-modal distribution.
Instead of being "gathered" around the average, the datapoints split into two seperate lines, with the average falling somewhere in between.

Also take a look at the following:

![scatter](/assets/images/2022-08-10-summer-of-code-2022/speedup_with_scatter2.png)

On the left-most side, we can observe a peculiar step-like pattern (which is probably caused by a quantized grain-size policy).  

Such observations would be missed if we only viewed the averaged line, and they can lead us to points of interest. Afterwards, we can examine (using debugging and profiling) what makes good runs run well, and what makes bad runs run slow.


### Task visualization

Viewing how parallel tasks are distributed ("scheduled") to available threads, can also be very useful for troubleshooting and improving performance.
We used Intel VTune, which was already partly incorporated in HPX.

Take a look at the example below:

![vtune1](/assets/images/2022-08-10-summer-of-code-2022/vtune1.png)

In the above proccess, we have two types of tasks, <span style="color:yellow">F1</span> (yellow) and <span style="color:LightGreen">F3</span> (green). Due to the nature of this implementation, **<span style="color:LightGreen">F3</span> tasks have to wait for all <span style="color:yellow">F1</span> tasks to finish before starting**.  
That is a source of inefficiency, because the first three cores aren't doing any useful work until the last core finishes with the last <span style="color:yellow">F1</span> task (red arrows).

We can try fixing this:

![vtune2](/assets/images/2022-08-10-summer-of-code-2022/vtune2.png)

We changed the implementation, such that <span style="color:LightGreen">F3</span> tasks don't have to wait for all <span style="color:yellow">F1</span> tasks. Thus, cores aren't left starving for work.

We can also distribute the work more evenly, so that all cores finish working together:

![vtune3](/assets/images/2022-08-10-summer-of-code-2022/vtune3.png)

> Note:  
> Those changes actually made the implementation run **slower** in some cases, because of additional overhead!  

This was just an example, but I hope it has given you a general idea of the project's scope and experimental nature.


## Links


## Future Work

## NOT COMPLETED
To be completed

[GSOC]: https://summerofcode.withgoogle.com/
[Stellar]: https://stellar-group.org/
[HPX]: https://github.com/STEllAR-GROUP/hpx